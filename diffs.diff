34a35,51
> \usepackage{listings}
> \usepackage{textcomp}
> \lstset{
>         basicstyle=\ttfamily\scriptsize,
>         upquote=true,
>         showspaces=false,
>         showstringspaces=false,
>         showtabs=false,
>         tabsize=2,
>         frame=none,
>         breaklines,
>         numbers=none,
>         framexleftmargin=2mm,
>         xleftmargin=2mm,
> }
> 
> 
88a106,111
> \and
> \alignauthor Davy Van Deursen\\
>        \affaddr{Ghent University - IBBT}\\
>        \affaddr{ELIS - Multimedia Lab}\\
>        \affaddr{Ghent, Belgium}\\
>        \email{davy.vandeursen@ugent.be}
91c114
<        \affaddr{ALBCOM and LSI Dept.}\\
---
>        \affaddr{Department LSI}\\
108a132
> % DVD: I would rather choose LOD or Linked Data, not both
112c136,139
< With SemWebVid \cite{Steiner:SemWebVid} we introduced a client-side interactive Ajax application for the automatic generation of RDF video annotations. For this paper we have re-implemented and improved the annotation logic on the server-side, resulting in a RESTful read/write-enabled Web service for RDF video annotations. A YouTube video is described by a Google Data Atom feed\footnote{E.g., \url{http://gdata.youtube.com/feeds/api/videos/Rq1dow1vTHY}}. In order to semantically annotate the various elements of this feed, we concentrate on the following fields (in XPath syntax): title (\texttt{/entry/media:group/media:title}), description (\texttt{/entry/media:group/media:description}), and tags (\texttt{/entry/media:group/media:keywords}). YouTube offers an automatic audio transcription service and video owners can also upload audio transcriptions and/or closed captions files on their own. We differentiate between subtitles and closed captions, where subtitles are hard-encoded into the video, and closed captions separate resources. Audio transcriptions consist of a plaintext representation of speech, however, without time information. In the case of audio transcriptions, YouTube automatically adds the time information and tries to convert them to closed captions. YouTube offers support for closed captions in several languages. Hence, in addition to the previously mentioned elements of the Google Data Atom feed, we thus also use closed captions\footnote{E.g., \url{http://www.youtube.com/watch_ajax?action_get_caption_track_all&v=Rq1dow1vTHY}} when they are available.
---
> % DVD: general remark regarding the introduction: it does not contain any problem statements and/or reasons why this paper is relevant for LDOW. So I would suggest to put a bit more focus on the actual problem that we want to solve (i.e., merging different enrichment services and how to deal with provenance information and why provenance information could be relevant). After that, you can start introducing the methodology.
> % DVD: is SemWebVid only for YouTube videos? If so, this should be stated and maybe also reflected in the title of the paper
> % DVD: the timing information present within the subtitles is a bit confusing to me: apparently, YouTube can do automatic transcriptions and adds during that process timing information. However, you do state also that 'Audio transcriptions consist of a plaintext representation of speech, however, without timing information'. Is this the case when the transcription is manually added? Also, if no timing information is present, is that a problem that is dealt with in this paper or not?
> With SemWebVid \cite{Steiner:SemWebVid} we introduced a client-side interactive Ajax application for the automatic generation of RDF video annotations. For this paper we have re-implemented and improved the annotation logic on the server-side, resulting in a RESTful read/write-enabled Web service for RDF video annotations. A YouTube video is described by a Google Data Atom feed\footnote{E.g., \url{http://gdata.youtube.com/feeds/api/videos/Rq1dow1vTHY}}. In order to semantically annotate the various elements of this feed, we concentrate on the following fields (in XPath syntax): title ({\tt /entry/media:group/media:title}), description ({\tt /entry/media:group/media:description}), and tags ({\tt /entry/media:group/media:keywords}). YouTube offers an automatic audio transcription service and video owners can also upload audio transcriptions and/or closed captions files on their own. We differentiate between subtitles and closed captions, where subtitles are hard-encoded into the video, and closed captions are separate resources. Audio transcriptions consist of a plaintext representation of speech, however, without timing information. In the case of audio transcriptions, YouTube automatically adds the timing information and tries to convert them to closed captions. YouTube offers support for closed captions in several languages. Hence, in addition to the previously mentioned elements of the Google Data Atom feed, we thus also use closed captions\footnote{E.g., \url{http://www.youtube.com/watch_ajax?action_get_caption_track_all&v=Rq1dow1vTHY}} when they are available.
114c141
< The remainder of this paper is structured as follows: section \ref{sec:services} introduces two classes of Web services that allow for unstructured data to be converted into Linked Data, section \ref{sec:consolidation} explains our approach to entity consolidation for URI Lookup and NLP Web services, section \ref{sec:tracking} contains a description of how we automatically maintain provenance metadata in our Web service, section \ref{sec:related} discusses related and future work, and finally section \ref{sec:conclusion} finalizes the paper with a conclusion.
---
> The remainder of this paper is structured as follows: Sect.~\ref{sec:services} introduces two classes of Web services that allow for unstructured data to be converted into Linked Data, Sect.~\ref{sec:consolidation} explains our approach to entity consolidation for URI Lookup and NLP Web services, Sect.~\ref{sec:tracking} contains a description of how we automatically maintain provenance metadata in our Web service, Sect.~\ref{sec:related} discusses related and future work, and finally Sect.~\ref{sec:conclusion} finalizes the paper with a conclusion.
117c144
< We differentiate between Natural Language Processing (NLP) Web services and URI Lookup Web services. The NLP Web services that we use for our experiments take a text fragment as an input, perform Named Entity Extraction (NEE) on it and them link extracted entities back into the Linked Open Data (LOD) cloud\footnote{\url{http://lod-cloud.net/}}. For NLP Web services we use OpenCalais, Zemanta, and AlchemyAPI\footnote{\url{http://www.opencalais.com/documentation/calais-web-service-api/}, \url{http://developer.zemanta.com/docs/}, \url{http://www.alchemyapi.com/api/entity/}}.
---
> We differentiate between Natural Language Processing (NLP) Web services and URI Lookup Web services. The NLP Web services that we use for our experiments take a text fragment as an input, perform Named Entity Extraction (NEE) on it and then link the extracted entities back into the Linked Open Data (LOD) cloud\footnote{\url{http://lod-cloud.net/}}. For NLP Web services we use OpenCalais, Zemanta, and AlchemyAPI\footnote{\url{http://www.opencalais.com/documentation/calais-web-service-api/}, \url{http://developer.zemanta.com/docs/}, \url{http://www.alchemyapi.com/api/entity/}}.
118a146
> % DVD: is there a reason that you not use the DBPedia SPARQL endpoint?
121,122c149,150
< \section{Entity Consolidation (EC)}\label{sec:consolidation}
< We define the process of entity consolidation (EC) as the merge of entities, i.e., if two services extract the same entity from the same input text fragment or term, we say that the entity is consolidated.
---
> \section{Entity Consolidation}\label{sec:consolidation}
> We define the process of entity consolidation as the merge of entities, i.e., if two services extract the same entity from the same input text fragment or term, we say that the entity is consolidated.
124a153
> % DVD: is there a rationale behind this approach? (i.e., the order of analysis)
127c156
< In section \ref{sec:consolidation1} we present our approach for how to consolidate entities from URI Lookup Web services, followed by section \ref{sec:consolidation2} where we show our approach for NLP Web services.
---
> In Sect.~\ref{sec:consolidation1} we present our approach for how to consolidate entities from URI Lookup Web services, followed by Sect.~\ref{sec:consolidation2} where we show our approach for NLP Web services.
129,131c158,160
< \subsection{EC For URI Lookup Web Services}\label{sec:consolidation1}
< As as first step we have implemented a wrapper for all four URI Lookup services that assimilates the particular service's output to a common output format. This format is the least common multiple of the information of all Web service results. For our experiments we agreed on the JSON format below (the examples below use the term \texttt{Google Translate} to illustrate the approach):
< \begin{verbatim}
---
> \subsection{Entity Consolidation For URI Lookup Web Services}\label{sec:consolidation1}
> As as first step we have implemented a wrapper for all four URI Lookup services that assimilates the particular service's output to a common output format. This format is the least common multiple of the information of all Web service results. For our experiments we agreed on the JSON format below (the examples below use the term ``Google Translate'' to illustrate the approach):
> \begin{lstlisting}
142,143c171,177
< \end{verbatim} 
< The corresponding request to our wrapper API that calls all four URI Lookup Web services in the background is via \texttt{GET} \url{/uri-lookup/combined/Google%20Translate} (the particular results from each service are available at \url{/uri-lookup/{service_name}/Google%20Translate}). As can be seen in the example above, already at the lowest data representation level (JSON) we maintain provenance metadata (Sindice in the concrete case of the example delivers a different result, and is thus not in the provenance list). In order to agree on a winner entity, a majority-based voting system is used. The problem, however, is that both Freebase and Uberblic return results in their own namespaces (e.g., for \texttt{Google Translate} the results are \url{http://freebase.com/en/google_translate}, and \url{http://uberblic.org/resource/67dc7037-6ae9-406c-86ce-997b905badc8#thing}), whereas Sindice and DBpedia Lookup return results from DBpedia (obvious for DBpedia Lookup, and from DBpedia among also other results for Sindice). Freebase and Uberblic interlink their results with DBpedia at an \texttt{owl:sameAs} level in the case of Freebase, and by referencing the source (\texttt{umeta:source\_uri}) for Uberblic, so by retrieving and parsing the referenced resources in the services' namespaces we can map back to DBpedia URIs and thus match all four services' results on the DBpedia level. Each service's result contributes with a relevance of 0.25 to the final result, in the above example when three services agree on the same result, the resulting relevance is thus the sum of the singular relevance scores (0.75 in this case).
---
> \end{lstlisting} 
> 
> % DVD: wouldn't it be better to rename 'provenance' to 'sources' (or 'data-sources')? Also, provenance information is typically information regarding the difference between two versions of the same data (i.e., which processes/agents/software was used and when and for what reason).
> 
> 
> % DVD: I separated this into 2 paragraphs: explain the data format first, then explain how it is filled in
> The corresponding request to our wrapper API that calls all four URI Lookup Web services in the background is via \texttt{GET} \url{/uri-lookup/combined/Google%20Translate} (the particular results from each service are available at \url{/uri-lookup/{service_name}/Google%20Translate}). As can be seen in the example above, already at the lowest data representation level (JSON) we maintain provenance metadata (Sindice in the concrete case of the example delivers a different result, and is thus not in the provenance list). 
145c179,183
< \subsection{EC For NLP Web Services}\label{sec:consolidation2}
---
> % DVD: regarding the different namespaces: basically, the problem is that different URIs are used for the same thing. One thing that you could do is the follow-your-nose strategy, as you described. Complementary, services such as SameAs (http://sameas.org/) could be helpful as well (is there a reason you don't use it?). I would suggest that we first state the basic problem and solution, and then give an example (now you kind of explain it via example which I find confusing after a first read).
> In order to agree on a winner entity, a majority-based voting system is used. The problem, however, is that both Freebase and Uberblic return results in their own namespaces (e.g., for ``Google Translate'' the results are \url{http://freebase.com/en/google_translate}, and \url{http://uberblic.org/resource/67dc7037-6ae9-406c-86ce-997b905badc8#thing}), whereas Sindice and DBpedia Lookup return results from DBpedia (obvious for DBpedia Lookup, and from DBpedia among also other results for Sindice). Freebase and Uberblic interlink their results with DBpedia at an \url{owl:sameAs} level in the case of Freebase, and by referencing the source (\url{umeta:source_uri}) for Uberblic. So by retrieving and parsing the referenced resources in the services' namespaces we can map back to DBpedia URIs and thus match all four services' results on the DBpedia level. Each service's result contributes with a relevance of 0.25 to the final result, in the above example when three services agree on the same result, the resulting relevance is thus the sum of the singular relevance scores (0.75 in this case).
> 
> 
> \subsection{Entitiy Consolidation For NLP Web Services}\label{sec:consolidation2}
147c185,188
< \begin{verbatim}
---
> 
> % DVD: same remark as above regarding the use of the term 'provenance'
> 
> \begin{lstlisting}
165,166c206,210
< \end{verbatim}
< These results come from a request to our wrapper API via \texttt{GET} \url{/entity-extraction/combined/Google%20Translate}, and as with URI Lookup the particular services' results can be obtained at \url{/entity-extraction/{service_name}/Google%20Translate}. While AlchemyAPI and Zemanta return results from DBpedia and other interlinked LOD cloud resources, OpenCalais returns only results in its own namespace (e.g., \url{http://d.opencalais.com/er/company/ralg-tr1r/ce181d44-1915-3387-83da-0dc4ec01c6da.rdf} for the company Google). While in that particular case retrieving the resource RDF representation and parsing for \texttt{owl:sameAs} links to DBpedia is successful, in general we found OpenCalais URIs sometimes point to non-existant resources, or to not very rich resources like \url{http://d.opencalais.com/pershash-1/cfcf1aa2-de05-3939-a7d5-10c9c7b3e87b.html} for the current US President Barack Obama, where the only information is that Barack Obama is of type person. In order to consolidate extracted entities, we use the following approach: we have a look at each of the extracted entities from service one and compare each entity's URIs with each URIs from each extracted entity from service two. To illustrate this, see the examples below (shortened for the sake of legibility, the used text fragment contains a reference to the company Google).
---
> \end{lstlisting}
> 
> % DVD: it is not clear to me what you expect from an NLP Web Service. Are you happy with the just the entity, or do you also want the type(s) of the entity, or do you want a full semantic description of the entity (in case of the latter, what's the remaining role of a URI lookup Web service then?). This section should answer these questions first, then discuss the format/API, then discuss the algorithm, then illustrate with the example.
> 
> These results come from a request to our wrapper API via \texttt{GET} \url{/entity-extraction/combined/Google%20Translate}, and as with URI Lookup the particular services' results can be obtained at \url{/entity-extraction/{service_name}/Google%20Translate}. While AlchemyAPI and Zemanta return results from DBpedia and other interlinked LOD cloud resources, OpenCalais returns only results in its own namespace (e.g., \url{http://d.opencalais.com/er/company/ralg-tr1r/ce181d44-1915-3387-83da-0dc4ec01c6da.rdf} for the company Google). While in that particular case retrieving the resource RDF representation and parsing for \texttt{owl:sameAs} links to DBpedia is successful, in general we found OpenCalais URIs sometimes point to non-existant resources, or to not very rich resources like \url{http://d.opencalais.com/pershash-1/cfcf1aa2-de05-3939-a7d5-10c9c7b3e87b.html} for the current US President Barack Obama (where the only information is that Barack Obama is of type person). In order to consolidate extracted entities, we use the following approach: we have a look at each of the extracted entities from service one and compare each entity's URIs with each URIs from each extracted entity from service two. To illustrate this, see the examples below (shortened for the sake of legibility, the used text fragment contains a reference to the company Google).
169c213
< \begin{verbatim}
---
> \begin{lstlisting}
189c233,234
< \end{verbatim}
---
> \end{lstlisting}
> 
191c236
< \begin{verbatim}
---
> \begin{lstlisting}
211,212c256,258
< \end{verbatim}
< As can be seen the entity names mismatch (\texttt{google inc.} vs. \texttt{google}), however, going down the list of URIs for the entity, one can note a match via \url{http://dbpedia.org/resource/Google}. In addition to that one can also see two would-be matches (\url{http://cb.semsol.org/company/google.rdf} vs. \url{http://cb.semsol.org/company/google#self} and \url{http://rdf.freebase.com/ns/en/google} vs. \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea}), however, because of the inconsistent use of URIs when there is more than one URI available for the same entity hinders the match from being made. An additional retrieval of the resources would be necessary to detect that in the latter case \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea} redirects to \url{http://rdf.freebase.com/ns/en/google}, whereas the first example seems to be broken (\url{http://cb.semsol.org/company/google#self} returns status code 404). The good thing, however, is that as soon as one match has been detected, one can consolidate the entities from both services. Again note how the two entity names mismatch (\texttt{google inc.} vs. \texttt{google}). The consolidated name is then an array of all detected synonymous names. The consolidated relevance is the average relevance of both services. In contrast to URI Lookup where we had to manually assign a relevance of 0.25 to each result because not all URI Lookup services include the concept of relevance in their results, with NLP services each service already includes the relevance concept for all services on a scale from 0 (irrelevant) to 1 (relevant), so we can directly use it. In our code the consolidated and merged entities from service one and two are then in turn compared to extracted entities from service three (and so on, if we used even more services), in practice, however, due to the not always given interconnectedness of OpenCalais, there are no matches after having compared Zemanta-extracted entities with AlchemyAPI-extracted entities. As above with URI Lookup-detected entity consolidation, also with NLP-detected entity consolidation we maintain provenance metadata for each URI on the lowest data representation level (JSON) on both a per URI basis and an entity basis.  
---
> \end{lstlisting}
> 
> As can be seen the entity names mismatch (``google inc.'' vs. ``google''), however, going down the list of URIs for the entity, one can note a match via \url{http://dbpedia.org/resource/Google}. Additionally, one can also see two would-be matches (\url{http://cb.semsol.org/company/google.rdf} vs. \url{http://cb.semsol.org/company/google#self} and \url{http://rdf.freebase.com/ns/en/google} vs. \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea}). However, because of the inconsistent use of URIs when there is more than one URI available for the same entity hinders the match from being made. An additional retrieval of the resources would be necessary to detect that in the latter case \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea} redirects to \url{http://rdf.freebase.com/ns/en/google}, whereas the first example seems to be broken (\url{http://cb.semsol.org/company/google#self} returns status code 404). The good thing, however, is that as soon as one match has been detected, one can consolidate the entities from both services. 
214c260
< It is to be noted that the results from URI Lookup are a subset of the results from NLP in our case, however, while all URI Lookup services accept one-word arguments (e.g., \texttt{google} works), from the NLP services only AlchemyAPI accepts one-word arguments, the two other services accept only non-trivial text fragments (e.g., \texttt{google is a company founded by larry page} works). We are, however, considering to further improve our approach to consilidate entities by using NLP and URI Lookup Web services in parallel for non-trivial terms (i.e., tags consisting of more than one word).
---
> Again note how the two entity names mismatch (``google inc.'' vs. ``google''). The consolidated name is then an array of all detected synonymous names. The consolidated relevance is the average relevance of both services. In contrast to URI Lookup where we had to manually assign a relevance of 0.25 to each result because not all URI Lookup services include the concept of relevance in their results, with NLP services each service already includes the relevance concept for all services on a scale from 0 (irrelevant) to 1 (relevant), so we can directly use it. In our approach the consolidated and merged entities from service one and two are then in turn compared to extracted entities from service three (and so on, if we used even more services). In practice, however, due to the not always given interconnectedness of OpenCalais, there are no matches after having compared Zemanta-extracted entities with AlchemyAPI-extracted entities. As above with URI Lookup-detected entity consolidation, also with NLP-detected entity consolidation we maintain provenance metadata for each URI on the lowest data representation level (JSON) on both a per URI basis and an entity basis.  
216c262,265
< \subsection{Design Of the Web Service}\label{sec:design}
---
> It is to be noted that the results from URI Lookup are a subset of the results from NLP in our case. However, while all URI Lookup services accept one-word arguments (e.g., ``google'' works), from the NLP services only AlchemyAPI accepts one-word arguments; the two other services accept only non-trivial text fragments (e.g., ``google is a company founded by larry page'' works). 
> 
> \subsection{Design of the Web Service}\label{sec:design}
> % DVD: will you publish the URI where we and reviewers can test the web service?
224,228c273,277
< \item Getting metadata from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/videos/{video_id}}
< \item Getting all closed captions from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/videos/{video_id}/closedcaptions}
< \item Getting closed captions in a given language from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/videos/{video_id}/closedcaptions/{language_code}}
< \item Getting a plaintext audio transcription from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/videos/{video_id}/audiotranscriptions}
< \item Getting a plaintext audio transcription in a given language from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/videos/{video_id}/audiotranscriptions/{language_code}}
---
> \item Getting metadata from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}}
> \item Getting all closed captions from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/closedcaptions}
> \item Getting closed captions in a given language from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/closedcaptions/{language_code}}
> \item Getting a plaintext audio transcription from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/audiotranscription}
> \item Getting a plaintext audio transcription in a given language from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/audiotranscription/{language_code}}
229a279,280
> 
> 
234c285
< We decided to use the W3C Ontology for Media Resources\cite{W3C:MediaOntology} as the central vocabulary, mainly because it already has a defined mapping not only for YouTube metadata, but also for many other existing metadata formats. "The ontology is supposed to foster the interoperability among various kinds of metadata formats currently used to describe media resources on the Web" (quoted from the introduction of \cite{W3C:MediaOntology}). From the vocabulary we use the following fields: \texttt{ma:title}, \texttt{ma:creator}, \texttt{ma:createDate}, and \texttt{ma:description}, which, as outlined before, have direct mappings to YouTube metadata. 
---
> We decided to use the W3C Ontology for Media Resources~\cite{W3C:MediaOntology} as the central vocabulary, mainly because it already has a defined mapping not only for YouTube metadata, but also for many other existing metadata formats. ``The ontology is supposed to foster the interoperability among various kinds of metadata formats currently used to describe media resources on the Web'' (quoted from the introduction of~\cite{W3C:MediaOntology}). From the vocabulary we use the following fields: \url{ma:title}, \url{ma:creator}, \url{ma:createDate}, and \url{ma:description}, which, as outlined before, have direct mappings to YouTube metadata. 
237c288
< In order to represent YouTube tags, or rather, semantically annotated YouTube tags, we use the Common Tag\cite{CommonTag:Spec} vocabulary. A resource is \texttt{ctag:tagged} with a \texttt{ctag:Tag}, which consists of a textual \texttt{ctag:label} and a pointer to a resource that specifies what the label \texttt{ctag:means}. The Common Tag vocabulary is well-established and developed by both industry and academic partners.
---
> In order to represent YouTube tags, or rather, semantically annotated YouTube tags, we use the Common Tag vocabulary~\cite{CommonTag:Spec}. A resource is \url{ctag:tagged} with a \url{ctag:Tag}, which consists of a textual \url{ctag:label} and a pointer to a resource that specifies what the label \url{ctag:means}. The Common Tag vocabulary is well-established and developed by both industry and academic partners.
240c291,295
< As stated before the current video annotation Web service is a re-implementation of our previous client-side application SemWebVid\cite{Steiner:SemWebVid}. Hence we already could collect some experience with modeling video data in RDF on our own, and were also inspired by the semantic video search engine yovisto\footnote{\url{http://yovisto.com/}}\cite{Sack:Use}, \cite{Sack:VideoSearch}. In our first attempt we used the Event Ontology\cite{Raimond:Event} and defined each line in the closed captions track as an \texttt{event:Event}. In the current implementation we simplified the annotation model by removing the notion of events, and by introducing the notion of video fragment instead. A video fragment stretches now over a complete sentence which usually contains more than just one line in the closed captions track, which much more matches the human perception of a self-contained incident in a video. In order to address a video fragment, we decided to use Media Fragment URIs\cite{W3C:MediaFrags}. Media Fragment URIs are also supported by the Ontology for Media Annotation in form of \texttt{ma:fragment}. In particular we use the temporal dimension (e.g., \url{http://example.org/video.webm#t=10,20}), which is defined by its start time and its end time relative to the entire video play time. In addition to the temporal dimension, we also use the track dimension (e.g., \url{http://example.org/video.webm#track=closedcaptions}), which allows for addressing only a closed captions track (i.e., speech with time information), or even the plaintext audio transcriptions without time information (e.g., \texttt{\#track=audiotranscriptions}). The value of the parameter \texttt{track} is a free-form string, so we are flexible with regards to its usage.
---
> % DVD: the sentence 'A video fragment stretches now over a complete sentence which usually contains more than just one line in the closed captions track, which much more matches the human perception of a self-contained incident in a video' is not clear to me.
> 
> As stated before the current video annotation Web service is a re-implementation of our previous client-side application SemWebVid~\cite{Steiner:SemWebVid}. Hence we already could collect some experience with modeling video data in RDF on our own, and were also inspired by the semantic video search engine yovisto\footnote{\url{http://yovisto.com/}}~\cite{Sack:Use, Sack:VideoSearch}. In our first attempt we used the Event Ontology\cite{Raimond:Event} and defined each line in the closed captions track as an \url{event:Event}. In the current implementation we simplified the annotation model by removing the notion of events, and by introducing the notion of video fragments instead. A video fragment stretches now over a complete sentence which usually contains more than just one line in the closed captions track, which much more matches the human perception of a self-contained incident in a video. 
> 
> In order to address a video fragment, we decided to use Media Fragment URIs~\cite{W3C:MediaFrags}. Media Fragment URIs are also supported by the Ontology for Media Annotation in form of \url{ma:MediaFragment}. In particular we use the temporal dimension (e.g., \url{http://example.org/video.webm#t=10,20}), which is defined by its start and end time relative to the entire video play time. In addition to the temporal dimension, we also use the track dimension (e.g., \url{http://example.org/video.webm#track=closedcaptions}), which allows for addressing only a closed captions track (i.e., speech with time information), or even the plaintext audio transcription without time information (e.g., \texttt{\#track=audiotranscription}). The value of the parameter \texttt{track} is a free-form string, so we are flexible with regards to its usage.
242,249c297,306
< In the previous SemWebVid implementation we used \texttt{event:factor}, \texttt{event:product}, and \texttt{event:agent} to relate events with factors (extracted non-person entities), products (the particular plaintext closed captions line), and agents (extracted persons). Now in order to annotate entities in a temporal video fragment, we consistently use the same vocabulary of Common Tag as outlined in section \ref{sec:youtube}. We thus have (in Turtle\footnote{\url{http://www.w3.org/TeamSubmission/turtle/}} syntax, left out prefixes for the sake of brefity):
< \begin{verbatim}
< <http://example.org/video.webm#t=10,20> a ma:fragment ;
<   ctag:tagged :tag .
< :tag a ctagTag ;
<   ctag:label "example" ;
<   ctag:means <http://example.org/example#> .
< \end{verbatim}
---
> In the previous SemWebVid implementation we used \url{event:factor}, \url{event:product}, and \url{event:agent} to relate events with factors (extracted non-person entities), products (the particular plaintext closed captions line), and agents (extracted persons). Now, in order to annotate entities in a temporal video fragment, we consistently use the same vocabulary of Common Tag as outlined in Sect.~\ref{sec:youtube}. We thus have (in Turtle\footnote{\url{http://www.w3.org/TeamSubmission/turtle/}} syntax, left out prefixes for the sake of brefity):
> \begin{lstlisting}
> <http://example.org/video.webm#t=10,20> 
>   a ma:MediaFragment ;
>   ctag:tagged 
>     [ a ctagTag ;
>       ctag:label "example" ;
>       ctag:means <http://example.org/example#>
>     ] .
> \end{lstlisting}
252,253c309,312
< As outlined before we use several data sources (Web services) in the background in order to deploy our own video annotation Web service. The simple example fact produced by our service that an \texttt{ma:fragment} is \texttt{ctag:tagged} with a \texttt{ctagTag} with the \texttt{ctag:label} in plaintext form \texttt{example}, where what this \texttt{ctag:label} \texttt{ctag:means} is represented by an example entity with the URI \url{http://example.org/example#}, might in consequence have been the result of up to, in the concrete case, seven agreeing (or disagreeing) Web services. In order to track the contributions of the various sources, we decided to use the Provenance Vocabulary\cite{Hartig:Provenance} by Hartig and Zhao. Even if the direct requests of our Web service were made against our wrappers (as outlined in sections \ref{sec:consolidation1} and \ref{sec:consolidation2}), we still want to credit back the results to the original calls to the third party Web services. We have two basic cases that affect the RDF that describes the data provenance, requests per HTTP \texttt{GET} and requests per HTTP \texttt{POST}. All URI Lookup services that we use are \texttt{GET}-based, all of our NLP services are \texttt{POST}-based. In order to make statements about a bundle of triples, we group them in a named graph. We use the TriG\cite{Bizer:TriG} syntax. The example from above then looks like this:
< \begin{verbatim}
---
> As outlined before we use several data sources (Web services) in the background in order to deploy our own video annotation Web service. The simple example fact produced by our service that a \url{ma:MediaFragment} is \url{ctag:tagged} with a \url{ctagTag} with the \url{ctag:label} in plaintext form \url{example}, where what this \url{ctag:label} \url{ctag:means} is represented by an example entity with the URI \url{http://example.org/example#}, might in consequence have been the result of up to, in the concrete case, seven agreeing (or disagreeing) Web services. In order to track the contributions of the various sources, we decided to use the Provenance Vocabulary~\cite{Hartig:Provenance} by Hartig and Zhao. Even if the direct requests of our Web service were made against our wrappers (as outlined in Sect.~\ref{sec:consolidation1} and Sect.~\ref{sec:consolidation2}), we still want to credit back the results to the original calls to the third party Web services. 
> 
> We have two basic cases that affect the RDF describing the data provenance: requests per HTTP \texttt{GET} and requests per HTTP \texttt{POST}. All URI Lookup services that we use are \texttt{GET}-based, all of our NLP services are \texttt{POST}-based. In order to make statements about a bundle of triples, we group them in a named graph. We use the TriG~\cite{Bizer:TriG} syntax. The example from above then looks like this:
> \begin{lstlisting}
255,259c314,320
<   <http://example.org/video.webm#t=10,20> a ma:fragment ;
<     ctag:tagged :tag .
<   :tag a ctagTag ;
<     ctag:label "example" ;
<     ctag:means <http://example.org/example#> .
---
>   <http://example.org/video.webm#t=10,20> 
>     a ma:MediaFragment ;
>     ctag:tagged [
>       a ctagTag ;
>       ctag:label "example" ;
>       ctag:means <http://example.org/example#> 
>     ] .  
261c322,324
< \end{verbatim}
---
> \end{lstlisting}
> 
> 
263c326
< In the next paragraph we outline the required steps in order to make statements about the provenance of a group of triples contained in a named graph \texttt{:G} that were generated using several HTTP \texttt{GET} requests to third party Web services. The text below can be best understood by following the triples in Appendix \ref{sec:appendix}.
---
> In the next paragraph we outline the required steps in order to make statements about the provenance of a group of triples contained in a named graph \url{:G} that were generated using several HTTP \texttt{GET} requests to third party Web services. The text below can be best understood by following the triples in Appendix~\ref{sec:appendix}.
265,266c328,329
< First, we state that \texttt{:G} is both a \texttt{prv:DataItem} and obviously an \texttt{rdfg:Graph}. \texttt{:G} is \texttt{prv:createdBy} the process of a \texttt{prv:DataCreation}. This \texttt{prv:DataCreation} is \texttt{prv:performedBy} a \texttt{prv:NonHumanActor}, a 
< \texttt{prvTypes:DataCreatingService} to be precise. This service is \texttt{prv:operatedBy} us (\url{http://tomayac.com/thomas_steiner.rdf#me}. Time is often important for provenance, so the \texttt{prv:performedAt} date of the \texttt{prv:DataCreation} needs to be saved. During the process of the \texttt{prv:DataCreation} there are \texttt{prv:usedData}, which are \texttt{prv:retrievedBy} a \texttt{prv:DataAcess} that is \texttt{prv:performedAt} a certain time, and \texttt{prv:performedBy} a non-human actor (our Web service) that is \texttt{prv:operatedBy} us (\url{http://tomayac.com/thomas_steiner.rdf#me}. For the \texttt{prv:DataAccess} (there is one for each third party Web service involved) we \texttt{prv:accessedService} from a \texttt{prv:DataProvidingService} of which we \texttt{prv:accessedResource} at a certain \texttt{irw:WebResource}. Therefore we \texttt{prvTypes:exchangedHTTPMessage} which is an \texttt{http:Request} using \texttt{http:httpVersion} "1.1" and the \texttt{http:methodName} "GET".
---
> First, we state that \url{:G} is both a \url{prv:DataItem} and obviously an \url{rdfg:Graph}. \url{:G} is \url{prv:createdBy} the process of a \url{prv:DataCreation}. This \url{prv:DataCreation} is \url{prv:performedBy} a \url{prv:NonHumanActor}, a 
> \url{prvTypes:DataCreatingService} to be precise. This service is \url{prv:operatedBy} a human (\url{http://tomayac.com/thomas_steiner.rdf#me}). Time is often important for provenance, so the \url{prv:performedAt} date of the \url{prv:DataCreation} needs to be saved. During the process of the \url{prv:DataCreation} there are \url{prv:usedData}, which are \url{prv:retrievedBy} a \url{prv:DataAcess} that is \url{prv:performedAt} a certain time, and \url{prv:performedBy} a non-human actor (our Web service) that is \url{prv:operatedBy} a human (\url{http://tomayac.com/thomas_steiner.rdf#me}. For the \url{prv:DataAccess} (there is one for each third party Web service involved) we \url{prv:accessedService} from a \url{prv:DataProvidingService} of which we \url{prv:accessedResource} at a certain \url{irw:WebResource}. Therefore we \url{prvTypes:exchangedHTTPMessage} which is an \url{http:Request} using \url{http:httpVersion} ``1.1'' and the \url{http:methodName} ``GET''.
269,270c332,333
< Oftentimes completely automatically generated RDF video annotation files will need a bit of manual fine-tuning. In our RESTful Web service we have thus envisiond that not only a big archive of automatically generated video annotations gets built, but that also people can correct errors in the RDF interactively (or remove completely wrong video annotations). For the correction case this can be tracked using the Provenance Vocabulary as follows (let us assume we wanted to replace an unfriendly Freebase \texttt{ctag:means} resource of \url{<http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea>} with the friendlier variant \url{<http://rdf.freebase.com/rdf/en.google>}):
< \begin{verbatim}
---
> Oftentimes completely automatically generated RDF video annotation files will need a bit of manual fine-tuning. In our RESTful Web service we have thus envisiond that not only a big archive of automatically generated video annotations gets built, but that also people can correct errors in the RDF interactively (or remove completely wrong video annotations). For the correction case this can be tracked using the Provenance Vocabulary as follows (let us assume we wanted to replace an unfriendly Freebase \url{ctag:means} resource of \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea} with the friendlier variant \url{http://rdf.freebase.com/rdf/en.google}):
> \begin{lstlisting}
272,273c335,336
<   <http://gdata.youtube.com/feeds/api/videos/3PuHGKnboNY> ctag:tagged :tag_corrected .
<   :tag_corrected
---
>   <http://gdata.youtube.com/feeds/api/videos/3PuHGKnboNY> 
>   ctag:tagged [
276c339,340
<     ctag:means <http://rdf.freebase.com/rdf/en.google> ;
---
>     ctag:means <http://rdf.freebase.com/rdf/en.google> 
>   ]  
286,287c350,351
<       <http://tomayac.com/thomas_steiner.rdf#me> ;
<     ] ;
---
>       <http://tomayac.com/thomas_steiner.rdf#me> 
>     ] 
289,290c353,358
< \end{verbatim}
< Note how the \texttt{prv:DataCreation} no longer contains references to \texttt{prv:usedData}. Obviously the shown approach to identify a \texttt{prv:HumanActor} with her FOAF profile requires an authentication step, which might not always be wanted. One could think of a "John Doe"\footnote{\url{http://en.wikipedia.org/wiki/John_Doe}}-like anonymous pseudo-\texttt{prv:HumanActor}, or in the case of an intendedly non-anonymous \texttt{prv:HumanActor}, authentication methods like WebID\footnote{\url{http://www.w3.org/2005/Incubator/webid/charter}} could be used.
---
> \end{lstlisting}
> 
> Note how the \url{prv:DataCreation} no longer contains references to \url{prv:usedData}. Obviously the shown approach to identify a \url{prv:HumanActor} with her FOAF profile requires an authentication step, which might not always be wanted. One could think of a ``John Doe''\footnote{\url{http://en.wikipedia.org/wiki/John_Doe}}-like anonymous pseudo-\url{prv:HumanActor}, or in the case of an intendedly non-anonymous \url{prv:HumanActor}, authentication methods like WebID\footnote{\url{http://www.w3.org/2005/Incubator/webid/charter}} could be used.
> 
> \subsection{The Need For Providing Provenance Metadata}
> Hartig et al. mention in~\cite{ipaw10:olaf} some reasons that justify the need for provenance metadata, among those linked dataset replication and distribution on the Web with not necessarily always the same namespaces: based on the same source data, different copies of a linked dataset can be created with different degrees of interconnectedness by different publishers.
292,293c360
< \subsection{Need For Provenance Metadata}
< Hartig et al. mention in \cite{ipaw10:olaf} some reasons that justify the need for provenance metadata, among those linked dataset replication and distribution on the Web with not necessarily always the same namespaces: based on the same source data, different copies of a linked dataset can be created with different degrees of interconnectedness by different publishers.
---
> We add to this list the automatic conversion of legacy unstructured data to Linked Data with heuristics, where, as in our case, extracted entities while being consolidated and backed up by different data sources, might still be wrong. Especially with our ``mash-up''-like approach it is very desirable to be able to track back to the concrete source where a certain piece of information might have come from. This a) in order to correct the error at the root of our Web service (fighting the cause), b) in order to correct the concrete error in an RDF annotation (fighting the symptom), or c) probably the most important reason, to judge the trustworthiness and quality of a dataset.
295c362
< We add to this list the automatic conversion of legacy unstructured data to Linked Data with heuristics, where, as in our case, extracted entities while being consolidated and backed up by different data sources, might still be wrong. Especially with our "mash-up"-like approach it is very desirable to be able to track back to the concrete source where a certain piece of information might have come from. This a), in order to correct the error at the root of our Web service (fighting the cause), b), in order to correct the concrete error in an RDF annotation (fighting the symptom), or c), probably the most important reason, to judge the trustworthiness and quality of a dataset.
---
> % DVD: what if DBPedia or other data sets change over time? I.e., the whole discussion regarding dataset dynamics in the context of provenance comes in the picture here. Therefore, the data sources themselves should also provide provenance information in order to enable a real working quality checker.
298c365
< Related work includes Popcorn.js from the Mozilla Drumbeat project\cite{Drumbeat:Popcorn} that with its interactive Butter editor\footnote{\url{http://popcornjs.org/butter/}} allows for a video to be semantically annotated (a completely manual process). Based on a given video annotation the Popcorn.js script then pulls in multiple data feeds from the APIs of Google News, Wikipedia, Twitter, and Flickr in order to semantically enrich the video viewing experience. It also provides automatic machine translation from Google Translate, and attribution data from Creative Commons. The focus, however, is on the final visual video "mash-up", not on the actual annotation. Future work could be to offer an RDF-to-Popcorn.js wrapper service that would allow us to profit from the project's HTML5 video framework.
---
> Related work includes Popcorn.js from the Mozilla Drumbeat project~\cite{Drumbeat:Popcorn} that with its interactive Butter editor\footnote{\url{http://popcornjs.org/butter/}} allows for a video to be semantically annotated (a completely manual process). Based on a given video annotation the Popcorn.js script then pulls in multiple data feeds from the APIs of Google News, Wikipedia, Twitter, and Flickr in order to semantically enrich the video viewing experience. It also provides automatic machine translation from Google Translate, and attribution data from Creative Commons. The focus, however, is on the final visual video ``mash-up'', not on the actual annotation. Future work could be to offer an RDF-to-Popcorn.js wrapper service that would allow us to profit from the project's HTML5 video framework.
300c367
< In \cite{Sack:VideoSearch}, Waitelonis et al. address the problem of how to deploy exploratory search for video data by using semantic search technology for the yovisto video search engine. They show how exploratory search can be enriched by information from the LOD cloud in order to facilitate navigation in big video archives. Yovisto supports several ways to annotate a video with metadata: video-related, and video-time-related tags. Video-related tags are applied to the entire video and are entered by the initial video uploader, whereas video-time-related tags only apply to a certain point in the video. They can either be automatically extracted from the video on a certain timestamp (e.g., by analyzing the video images with OCR methods), or can be user-generated tags also on a certain timestamp. In a different paper \cite{Sack:Use} Waitelonis et al. show how using permutations of a term and this term's surrounding context and by detecting paths between entities, a legacy keyword-based video search engine can be converted into a semantic video search engine. The approach uses a keyword-to-DBpedia-URI mapping heuristic, however, as far as we can tell, provenance metadata is not maintained. Future work will compare the results of the yovisto heuristic with ours using agreed-on benchmarks.
---
> In~\cite{Sack:VideoSearch}, Waitelonis et al. address the problem of how to deploy exploratory search for video data by using semantic search technology for the yovisto video search engine. They show how exploratory search can be enriched by information from the LOD cloud in order to facilitate navigation in big video archives. Yovisto supports several ways to annotate a video with metadata: video-related, and video-time-related tags. Video-related tags are applied to the entire video and are entered by the initial video uploader, whereas video-time-related tags only apply to a certain point in the video. They can either be automatically extracted from the video on a certain timestamp (e.g., by analyzing the video images with OCR methods), or can be user-generated tags also on a certain timestamp. In a different paper~\cite{Sack:Use} Waitelonis et al. show how using permutations of a term and this term's surrounding context and by detecting paths between entities, a legacy keyword-based video search engine can be converted into a semantic video search engine. The approach uses a keyword-to-DBpedia-URI mapping heuristic, however, as far as we can tell, provenance metadata is not maintained. Future work will compare the results of the yovisto heuristic with ours using agreed-on benchmarks.
302c369
< In \cite{Choudhury:YouTube} Choudhury et al. describe a framework for semantic enrichment, ranking, and integration of Web video tags using Semantic Web technologies. In order to enrich the oftentimes sparse user-generated tag space, meta data like the recording time and location, or the video title and video description are used, but also social features such as playlists that a video appears in and related videos. Next, the tags are ranked by their co-occurrence and in a final step interlinked to DBpedia concepts for greater integration with other datasets. Choudhury et al. disambiguate the tags based on WordNet\footnote{\url{http://wordnet.princeton.edu/}} synsets if possible. That means if there is only one matching synset in WordNet, the corresponding WordNet URI in DBpedia is selected. If there are more than one matching synsets, the tags and their context tags similarity is computed and thereby tried to decide on an already existing tag URI. For words that are not contained in WordNet, Sindice is used to find the most probable concept. To the best of our knowledge provenance metadata is not maintained.
---
> In~\cite{Choudhury:YouTube} Choudhury et al. describe a framework for semantic enrichment, ranking, and integration of Web video tags using Semantic Web technologies. In order to enrich the oftentimes sparse user-generated tag space, meta data like the recording time and location, or the video title and video description are used, but also social features such as playlists that a video appears in and related videos. Next, the tags are ranked by their co-occurrence and in a final step interlinked to DBpedia concepts for greater integration with other datasets. Choudhury et al. disambiguate the tags based on WordNet\footnote{\url{http://wordnet.princeton.edu/}} synsets if possible. That means if there is only one matching synset in WordNet, the corresponding WordNet URI in DBpedia is selected. If there are more than one matching synsets, the tags and their context tags similarity is computed and thereby tried to decide on an already existing tag URI. For words that are not contained in WordNet, Sindice is used to find the most probable concept. To the best of our knowledge provenance metadata is not maintained.
305c372
< We have introduced a Web service for semantic text-based video annotation for YouTube videos with closed captions. Therefore we presented several URI Lookup and NLP Web services and showed our approach for both classes of Web services to consolidate entities. We then focused on the necessary RDF vocabularies and Media Fragment URIs to annotate video-related and video-time-related entities. Due to their different "mash-up"-like history of origins, we need to track provenance metadata in order to assure the trustworthiness of the generated data. We showed how the Provenance Vocabulary can be used to keep track of even the original third party Web service calls that led to consolidated results. It is to be noted that these references to the original calls are to be understood as the identificator of Web resources (i.e., the results of a request). Finally we positioned our work to related work, and presented directions for future work.
---
> We have introduced a Web service for semantic text-based video annotation for YouTube videos with closed captions. Therefore we presented several URI Lookup and NLP Web services and showed our approach for both classes of Web services to consolidate entities. We then focused on the necessary RDF vocabularies and Media Fragment URIs to annotate video-related and video-time-related entities. Due to their different ``mash-up''-like history of origins, we need to track provenance metadata in order to assure the trustworthiness of the generated data. We showed how the Provenance Vocabulary can be used to keep track of even the original third party Web service calls that led to consolidated results. It is to be noted that these references to the original calls are to be understood as the identificator of Web resources (i.e., the results of a request). Finally we positioned our work to related work, and presented directions for future work.
313c380
< We would like to thank Olaf Hartig from the Humboldt-Universit\"{a}t zu Berlin for his kind support with the correct use of the Provenance Vocabulary. J. Gabarr\'{o} is partially supported by TIN-2007-66523 (FORMALISM) and SGR 2009-2015 (ALBCOM). T. Steiner is partially funded by the EU FP7 I-SEARCH project (project reference 248296).
---
> We would like to thank Olaf Hartig from the Humboldt-Universit\"{a}t zu Berlin for his kind support with the correct use of the Provenance Vocabulary. This work is partly funded by the EU FP7 I-SEARCH project (project reference 248296).
329c396
< \pagebreak
---
> %\pagebreak
333,334c400,401
< Shortened overview of the provenance RDF in Turtle syntax for a YouTube tag with the label \texttt{obama} and the assigned meaning \url{http://dbpedia.org/resource/Barack_Obama} (for the sake of brefity only two of the \texttt{prv:usedData} sources are mentioned):
< \begin{verbatim}
---
> Shortened overview of the provenance RDF in Turtle syntax for a YouTube tag with the label ``obama'' and the assigned meaning \url{http://dbpedia.org/resource/Barack_Obama} (for the sake of brefity only two of the \url{prv:usedData} sources are mentioned):
> \begin{lstlisting}
403c470
< \end{verbatim}
---
> \end{lstlisting}

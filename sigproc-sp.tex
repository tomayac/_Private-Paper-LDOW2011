% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

%\usepackage{url}
\usepackage[hyphens]{url}
\usepackage{textcomp}

\usepackage{listings}
\usepackage{textcomp}
\lstset{
        basicstyle=\ttfamily\scriptsize,
        upquote=true,
        showspaces=false,
        showstringspaces=false,
        showtabs=false,
        tabsize=2,
        frame=none,
        breaklines,
        numbers=none,
        framexleftmargin=2mm,
        xleftmargin=2mm,
}


\begin{document}

\title{Tracking Provenance In An Entity-Consolidating Read/Write API For RDF YouTube Video Annotations}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor Thomas Steiner\\
       \affaddr{Univ. Polit{\'e}cnica de Catalunya}\\
       \affaddr{Department LSI}\\
       \affaddr{08034 Barcelona, Spain}\\
       \email{tsteiner@lsi.upc.edu}
\and
\alignauthor Rapha\"{e}l Troncy\\
       \affaddr{EURECOM}\\
       \affaddr{Sophia Antipolis}\\
       \affaddr{France}\\
       \email{raphael.troncy@eurecom.fr}
\and
\alignauthor Davy Van Deursen\\
       \affaddr{Ghent University - IBBT}\\
       \affaddr{ELIS - Multimedia Lab}\\
       \affaddr{Ghent, Belgium}\\
       \email{davy.vandeursen@ugent.be}
\and
\alignauthor Joaquim Gabarr\'{o}\\
       \affaddr{Univ. Polit{\'e}cnica de Catalunya}\\
       \affaddr{ALBCOM and LSI Dept.}\\
       \affaddr{08034 Barcelona, Spain}\\
       \email{gabarro@lsi.upc.edu}
}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Using Natural Language Processing or URI Lookup third party Web services, converting legacy unstructured data into Linked Data is a relatively straight-forward task. In this paper we present an approach to first consolidate entities found by such Web services when being used in parallel, and then describe how one can keep track of provenance at the same time. We have implemented a RESTful Web service for on-the-fly text-based RDF annotation of YouTube videos that illustrates how provenance metadata can be automatically added to the Web service output, and discuss how in our read/write-enabled Web service manual changes to automatically generated RDF annotations can be tracked. 
\end{abstract}

% A category with the (minimum) three required fields
\category{H.3}{Information Storage and Retrieval}{On-line Information Services}

\terms{Experimentation}

% DVD: I would rather choose LOD or Linked Data, not both
% TOM: Chose Linked Data
\keywords{RDF, Linked Data, Semantic Web, NLP, Video} % NOT required for Proceedings

\section{Introduction}\label{sec:introduction}
% DVD: general remark regarding the introduction: it does not contain any problem statements and/or reasons why this paper is relevant for LDOW. So I would suggest to put a bit more focus on the actual problem that we want to solve (i.e., merging different enrichment services and how to deal with provenance information and why provenance information could be relevant). After that, you can start introducing the methodology.
% TOM: Done. See below.
% DVD: is SemWebVid only for YouTube videos? If so, this should be stated and maybe also reflected in the title of the paper
% TOM: Currently yes. See the modified title to reflect this: "Tracking Provenance In An Entity-Consolidating Read/Write API For RDF YouTube Video Annotations".
% DVD: the timing information present within the subtitles is a bit confusing to me: apparently, YouTube can do automatic transcriptions and adds during that process timing information. However, you do state also that 'Audio transcriptions consist of a plaintext representation of speech, however, without timing information'. Is this the case when the transcription is manually added? Also, if no timing information is present, is that a problem that is dealt with in this paper or not?
% TOM: Reformulated and added more details in order to hopefully clarify.
With SemWebVid \cite{Steiner:SemWebVid} we introduced a client-side interactive Ajax application for the automatic generation of RDF video annotations. For this paper we have re-implemented and improved the annotation logic on the server-side, resulting in a RESTful read/write-enabled Web service for RDF video annotations. In the background this Web service calls several enrichment services in parallel and merges their results in order to consolidate entities. We present two linking algorithms for two different kinds of enrichment services: Natural Language Processing (NLP) Web services and URI Lookup Web services. This with the objective in mind to publish legacy data sources as Linked Data on the Web, with the option of a write-enabled back-channel for manual corrections. Upon merging results from different data sources, being able to track back provenance of Linked Data is essential in order to judge its trustworthiness and reliability. In this paper we propose an approach for maintaining trackable provenance metadata.
\subsection{Structure of YouTube Video Objects}\label{sec:youtubestructure}
A YouTube video is described by a Google Data Atom feed\footnote{See \url{http://gdata.youtube.com/feeds/api/videos/Rq1dow1vTHY} for a concrete example.}. In order to semantically annotate a YouTube video, we concentrate on the following fields of this feed (in XPath syntax): title ({\tt /entry/media:group/media:title}), description ({\tt /entry/media:group/media:description}), and tags ({\tt /entry/media:group/media:keywords}).
\subsection{On Closed Captions}\label{sec:closedcaptions}
In the following we differentiate first between subtitles and closed captions, where subtitles are hard-encoded into the video frames, and closed captions are separate textual resources. Currently we only work with closed captions. Closed captions are based on audio transcriptions, which consist of a plaintext representation of speech of a video, however, without timing information. When this timing information gets added, we have closed captions. YouTube offers an automatic audio transcription service and video owners can also upload audio transcriptions and/or closed captions files on their own. In the case of audio transcriptions\footnote{\url{http://googleblog.blogspot.com/2009/11/automatic-captions-in-youtube.html}}, YouTube automatically adds the timing information and tries to convert them to closed captions. YouTube supports closed captions in several languages. Hence, for our task of semantic video annotation, in addition to the previously mentioned elements of the Google Data Atom feed, we thus also use closed captions\footnote{See \url{http://www.youtube.com/watch_ajax?action_get_caption_track_all&v=Rq1dow1vTHY} for a concrete example.} when they are available.

The remainder of this paper is structured as follows: Sect.~\ref{sec:services} introduces two classes of Web services that allow for unstructured data to be converted into Linked Data, Sect.~\ref{sec:consolidation} explains our approach to entity consolidation for URI Lookup and NLP Web services, Sect.~\ref{sec:tracking} contains a description of how we automatically maintain provenance metadata in our Web service, Sect.~\ref{sec:related} discusses related and future work, and finally Sect.~\ref{sec:conclusion} finalizes the paper with a conclusion.

\section{Web services for converting unstructured data into Linked Data}\label{sec:services}
We differentiate between Natural Language Processing (NLP) Web services and URI Lookup Web services.
\subsection{NLP Web Services}\label{sec:nlp}
The NLP Web services that we use for our experiments take a text fragment as an input, perform Named Entity Extraction (NEE) on it and then link the extracted entities back into the Linked Open Data (LOD) cloud\footnote{\url{http://lod-cloud.net/}}. For NLP Web services we use OpenCalais, Zemanta, and AlchemyAPI\footnote{\url{http://www.opencalais.com/documentation/calais-web-service-api/},\\ \url{http://developer.zemanta.com/docs/},\\ \url{http://www.alchemyapi.com/api/entity/}}.

% DVD: is there a reason that you not use the DBPedia SPARQL endpoint?
% TOM: See ref{sec:technicalnote}
\subsection{URI Lookup Web Services}\label{sec:urilookup}
The URI Lookup Web services take a term as an input, and return the set of URIs that most probably represent this term. We use Freebase, DBpedia Lookup, Sindice, and Uberblic\footnote{\url{http://wiki.freebase.com/wiki/Search},\\ \url{http://lookup.dbpedia.org/},\\ \url{http://sindice.com/developers/api#SindicePublicAPI-TermSearch},\\ \url{http://uberblic.org/developers/apis/search/}}.

\subsection{Technical Note}\label{sec:technicalnote}
For both classes of services we use all services in parallel, aiming for the emergence effect in the sense of Aristotle\footnote{Aristotle, Metaphysics, Book H 1045a 8-10: "[\ldots] the totality is not, as it were, a mere heap, but the whole is something besides the parts [\ldots]"}. Our Web service is written in JavaScript and based on Node\footnote{\url{http://nodejs.org/}} and the Express\footnote{\url{http://expressjs.com/index.html}} framework, therefore we use the third parties' JSON or XML APIs as a means of communication with all services, and not the potential SPARQL endpoints or RDF APIs. In the next section we describe our strategies for entity consolidation in both cases. 

\section{Entity Consolidation}\label{sec:consolidation}
We define the process of entity consolidation as the merge of entities, i.e., if two services extract the same entity from the same input text fragment or term, we say that the entity is consolidated.

\subsection{Interplay Of the Web Services}\label{sec:interplay}
% DVD: is there a rationale behind this approach? (i.e., the order of analysis)
% TOM: Yepp. See the added comment.
As outlined before the metadata for a YouTube video are its title, its description, its tags, and its closed captions tracks. We analyze the tags one-by-one with URI Lookup Web services through our wrapper Web service, and afterwards we analyze the title combined with the description in a first, and the closed captions track in a second separate run through our wrapper Web service to the NLP Web services. This order was chosen because sometimes video description and closed captions are not related at all. This happens when the video description gets used not in a way to describe the video content on a high level, but, e.g., to advertise different videos from the same user. In this case the NLP gets steered in a completely wrong direction. During our experiments this occurred often enough for us to separate the NLP analysis of descriptions and closed captions into two independent steps. When all analysis steps are taken, the extracted entites from all classes of services are tried to be matched back in the video.

In Sect.~\ref{sec:consolidation1} we present our approach for how to consolidate entities from URI Lookup Web services, followed by Sect.~\ref{sec:consolidation2} where we show our approach for NLP Web services.

\subsection{Entity Consolidation For URI Lookup Web Services}\label{sec:consolidation1}
As as first step we have implemented a wrapper for all four URI Lookup services that assimilates the particular service's output to a common output format. This format is the least common multiple of the information of all Web service results. For our experiments we agreed on the JSON format below (the examples below use the term ``Google Translate'' to illustrate the approach): 
\begin{lstlisting}
[
  {
    "name": "Google Translate",
    "uris": [
      "http://dbpedia.org/resource/Google_Translate"
    ],
    "source": "freebase,uberblic,dbpedia",
    "relevance": 0.75
  }
]
\end{lstlisting} 

% DVD: wouldn't it be better to rename 'provenance' to 'sources' (or 'data-sources')? Also, provenance information is typically information regarding the difference between two versions of the same data (i.e., which processes/agents/software was used and when and for what reason).
% TOM: Agreed. Renamed to "source" globally.

% DVD: I separated this into 2 paragraphs: explain the data format first, then explain how it is filled in
% TOM: Makes total sense.
The corresponding request to our wrapper API that calls all four URI Lookup Web services in the background is via \texttt{GET} \url{/uri-lookup/combined/Google%20Translate} (the particular results from each service are available at \url{/uri-lookup/{service_name}/Google%20Translate}). As can be seen in the example above, already at the lowest data representation level (JSON) we maintain provenance metadata in the form of a \texttt{source} field (Sindice in the concrete case of the example delivers a different result, and is thus not in the source list). 

% DVD: regarding the different namespaces: basically, the problem is that different URIs are used for the same thing. One thing that you could do is the follow-your-nose strategy, as you described. Complementary, services such as SameAs (http://sameas.org/) could be helpful as well (is there a reason you don't use it?). I would suggest that we first state the basic problem and solution, and then give an example (now you kind of explain it via example which I find confusing after a first read).
% TOM: Agreed. Added an explanation at the beginning. With regards to the sameAs.org service, see \ref{sec:sameasorg}.
In order to agree on a winner entity, a majority-based voting system is used. As soon as for two entities only one of these entities' URIs match, we consider the entity consolidated and can merge the results. The problem, however, is that both Freebase and Uberblic return results in their own namespaces (e.g., for ``Google Translate'' the results are \url{http://freebase.com/en/google_translate}, and \url{http://uberblic.org/resource/67dc7037-6ae9-406c-86ce-997b905badc8#thing}), whereas Sindice and DBpedia Lookup return results from DBpedia (obvious for DBpedia Lookup, and from DBpedia among also other results for Sindice). Freebase and Uberblic interlink their results with DBpedia at an \url{owl:sameAs} level in the case of Freebase, and by referencing the source (\url{umeta:source_uri}) for Uberblic. So by retrieving and parsing the referenced resources in the services' namespaces we can map back to DBpedia URIs and thus match all four services' results on the DBpedia level. Each service's result contributes with a relevance of 0.25 to the final result, in the above example when three services agree on the same result, the resulting relevance is thus the sum of the singular relevance scores (0.75 in this case).

\subsection{Entitiy Consolidation For NLP Web Services}\label{sec:consolidation2}
In analogy to our approach to URI Lookup entity consolidation, we have implemented a wrapper API for the three NLP services. While the original calls to the particular NLP service are all HTTP \texttt{POST}-based, we have implemented the wrapper \texttt{GET}- and \texttt{POST}-based. All NLP Web services return detected types and/or subtypes, names, relevance, and URIs that link into the LOD cloud. The problem is that each service has implemented its own typing system, providing mappings for all of them would be a relatively time-consuming task. However, as all services provide links into the LOD cloud, the desired typing information can be pulled from there in a true Linked Data manner, and we have all the information we need if named entities with interlinked URIs are detected. The least common multiple of the particular results can thus be seen below (shortened to one entity with just two URIs for the sake of legibility, the original result contained seven entities, thereof six directly relevant, and one related, but not directly relevant entity):

% DVD: same remark as above regarding the use of the term 'provenance'
% TOM: Done.

\begin{lstlisting}
[ 
  {
    "name": "Google Translate",
    "relevance": 0.7128319999999999,
    "uris": [
      {
        "uri": "http://dbpedia.org/resource/Google_Translate",
        "source": "alchemyapi"
      },
      {
        "uri": "http://rdf.freebase.com/ns/en/google_translate",
        "source": "zemanta"
      }
    ],
    "source": "alchemyapi,zemanta"
  }
]
\end{lstlisting}

% DVD: it is not clear to me what you expect from an NLP Web Service. Are you happy with the just the entity, or do you also want the type(s) of the entity, or do you want a full semantic description of the entity (in case of the latter, what's the remaining role of a URI lookup Web service then?). This section should answer these questions first, then discuss the format/API, then discuss the algorithm, then illustrate with the example.
% TOM: Fair enough, this was not at all explained. I tried to explain now in a new introduction to the section, see above. With regards to the role of URI Lookup services: see the last paragraph of \ref{sec:consolidation2}

These results come from a request to our wrapper API via \texttt{GET} \url{/entity-extraction/combined/Google%20Translate}, and as with URI Lookup the particular services' results can be obtained at \url{/entity-extraction/{service_name}/Google%20Translate}. While AlchemyAPI and Zemanta return results from DBpedia and other interlinked LOD cloud resources, OpenCalais returns only results in its own namespace (e.g., \url{http://d.opencalais.com/er/company/ralg-tr1r/ce181d44-1915-3387-83da-0dc4ec01c6da.rdf} for the company Google). While in that particular case retrieving the resource RDF representation and parsing for \texttt{owl:sameAs} links to DBpedia is successful, in general we found OpenCalais URIs sometimes point to non-existant resources, or to not very rich resources like \url{http://d.opencalais.com/pershash-1/cfcf1aa2-de05-3939-a7d5-10c9c7b3e87b.html} for the current US President Barack Obama (where the only information is that Barack Obama is of type person). In order to consolidate extracted entities, we use the following approach: we have a look at each of the extracted entities from service one and compare each entity's URIs with each URIs from each extracted entity from service two. To illustrate this, see the examples below (shortened for the sake of legibility, the used text fragment contains a reference to the company Google).

Results for the text fragment from only AlchemyAPI:
\begin{lstlisting}
{
  "name": "Google",
  "relevance": 0.496061,
  "uris": [
    {
      "uri": "http://dbpedia.org/resource/Google",
      "source": "alchemyapi" 
    },
    {
      "uri": "http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea",
      "source": "alchemyapi" 
    },
    {
      "uri": "http://cb.semsol.org/company/google.rdf",
      "source": "alchemyapi" 
    } 
  ],
  "source": "alchemyapi" 
}
\end{lstlisting}

Results for the text fragment from only Zemanta:
\begin{lstlisting}
{
  "name": "Google Inc.",
  "relevance": 0.563132,
  "uris": [
    {
      "uri": "http://rdf.freebase.com/ns/en/google",
      "source": "zemanta" 
    },
    {
      "uri": "http://dbpedia.org/resource/Google",
      "source": "zemanta" 
    },
    {
      "uri": "http://cb.semsol.org/company/google#self",
      "source": "zemanta" 
    } 
  ],
  "source": "zemanta" 
}
\end{lstlisting}
Results merged from both Zemanta and AlchemyAPI:
\begin{lstlisting}
{
  "name": [
    "Google",
    "Google Inc."
  ],
  "relevance":  0.5295965,
  "uris": [
    {
      "uri": "http://dbpedia.org/resource/Google",
      "source": "alchemyapi" 
    },
    {
      "uri": "http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea",
      "source": "alchemyapi" 
    },
    {
      "uri": "http://umbel.org/umbel/ne/wikipedia/Google",
      "source": "alchemyapi" 
    },
    {
      "uri": "http://cb.semsol.org/company/google.rdf",
      "source": "alchemyapi" 
    },
    {
      "uri": "http://rdf.freebase.com/ns/en/google",
      "source": "zemanta" 
    },
    {
      "uri": "http://cb.semsol.org/company/google#self",
      "source": "zemanta" 
    } 
  ],
  "source": "alchemyapi,zemanta" 
}
\end{lstlisting}
As can be seen the entity names mismatch (``google inc.'' vs. ``google''), however, going down the list of URIs for the entity, one can note a match via \url{http://dbpedia.org/resource/Google}. Additionally, one can also see two would-be matches (\url{http://cb.semsol.org/company/google.rdf} vs. \url{http://cb.semsol.org/company/google#self} and \url{http://rdf.freebase.com/ns/en/google} vs. \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea}). However, the inconsistent use of URIs when there is more than one URI available for the same entity hinders the match from being made. An additional retrieval of the resources would be necessary to detect that in the latter case \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea} redirects to \url{http://rdf.freebase.com/ns/en/google}, whereas the first example seems to be broken (\url{http://cb.semsol.org/company/google#self} returns status code 404). The good thing, however, is that as soon as one match has been detected, one can consolidate the entities from both services. 

Again note how the two entity names mismatch (``google inc.'' vs. ``google''). The consolidated name is then an array of all detected synonymous names. The consolidated relevance is the average relevance of both services. In contrast to URI Lookup where we had to manually assign a relevance of 0.25 to each result because not all URI Lookup services include the concept of relevance in their results, with NLP services each service already includes a relevance concept for all services on a scale from 0 (irrelevant) to 1 (relevant), so we can directly use it. In our approach the consolidated and merged entities from service one and two are then in turn compared to extracted entities from service three (and so on, if we used even more services). In practice, however, due to the not always given interconnectedness of OpenCalais, there are no matches after having compared Zemanta-extracted entities with AlchemyAPI-extracted entities. As above with URI Lookup-detected entity consolidation, also with NLP-detected entity consolidation we maintain provenance metadata for each URI on the lowest data representation level (JSON) on both a per URI basis and an entity basis.  

It is to be noted that the results from URI Lookup are a subset of the results from NLP in our case. However, while all URI Lookup services accept one-word arguments (e.g., ``google'' works), from the NLP services only AlchemyAPI accepts one-word arguments; the two other services accept only non-trivial text fragments (e.g., ``google is a company founded by larry page'' works). 

\subsection{Identity Links On the Semantic Web}\label{sec:sameasorg}
In order to tackle the problem of different namespaces in results that we have outlined in Sect.~\ref{sec:consolidation1}, a straight-forward idea might be to use a Web service such as <sameAs>\footnote{\url{http://sameas.org/}} to easily find mappings from one namespace into another. In practice, however, while many data sources in the Linked Data world are marked as being equal to each other (e.g., \url{http://dbpedia.org/resource/Barack_Obama} \texttt{owl:sameAs} \url{http://rdf.freebase.com/rdf/en.barack_obama}), the quality of such equality links is not always excellent. As Halpin et al. show in \cite{Halpin:SameAs}, the problem with \texttt{owl:sameAs}, however, is that people tend to use it very differently. The authors of \cite{Halpin:SameAs} differentiate four separate usage styles, each with its particular implications. Inference is thus problematic, if not impossible, when the sense of the particular use of \texttt{owl:sameAs} is unknown.

\subsection{Design of the Web Service}\label{sec:design}
% DVD: will you publish the URI where we and reviewers can test the web service?
% TOM: As all this stuff is Node-based, I could not find an external hosting service yet (I signed up for Heroku and Joyent, but am still on the waiting list and I'm not sure if they will allow me to do all what I want to do). So short, it's localhost for the moment. Plus the code is not really rock-solid yet, but it works.
Our Web service is designed with RESTful design principles in mind: properly named resources, use of the adequate HTTP verbs, and implementation of Hypermedia Controls (also known as HATEOAS\footnote{\url{http://martinfowler.com/articles/richardsonMaturityModel.html#level3}}). Currently our Web service supports the following operations:
\begin{itemize}
\item Looking up URIs for a given term (allowed service names are ``dbpedia", ``freebase", ``uberblic", ``sindice", and ``combined"): \texttt{GET} \url{/uri-lookup/{service_name}/{term}} 
\item Extracting entities from a given text fragment (allowed service names are ``opencalais", ``zemanta", ``alchemyapi", and ``combined"): \texttt{GET} | \texttt{POST} \url{/entity-extraction/{service_name}/{text_fragment}}\footnote{In the case of \texttt{POST}, the \texttt{\{text\_fragment\}} has to be sent in the body of the HTTP message.}
\item Getting an RDF annotation for a video with a given video ID: \texttt{GET} \url{/youtube/rdf/{video_id}}
\item Modifying or manually creating an RDF annotation for a video with a given video ID: \texttt{PUT} \url{/youtube/rdf/{video_id}}
\item Deleting an RDF annotation for a video with a given video ID: \texttt{DELETE} \url{/youtube/rdf/{video_id}}
\item Getting metadata from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}}
\item Getting all closed captions from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/closedcaptions}
\item Getting closed captions in a given language from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/closedcaptions/{language_code}}
\item Getting a plaintext audio transcription from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/audiotranscription}
\item Getting a plaintext audio transcription in a given language from YouTube for a video with a given video ID: \texttt{GET} \url{/youtube/video/{video_id}/audiotranscription/{language_code}}
\end{itemize}

\section{Representing Videos in RDF}\label{sec:representing}
In the following we present the particular components of the available video metadata and their representation in RDF.

\subsection{Basic YouTube Metadata}\label{sec:metadata}
We decided to use the W3C Ontology for Media Resources~\cite{W3C:MediaOntology} as the central vocabulary, mainly because it already has a defined mapping not only for YouTube metadata, but also for many other existing metadata formats. ``The ontology is supposed to foster the interoperability among various kinds of metadata formats currently used to describe media resources on the Web'' (quoted from the introduction of~\cite{W3C:MediaOntology}). From the vocabulary we use the following fields: \url{ma:title}, \url{ma:creator}, \url{ma:createDate}, and \url{ma:description}, which, as outlined before, have direct mappings to YouTube metadata. 

\subsection{YouTube Tags}\label{sec:youtube}
In order to represent YouTube tags, or rather, semantically annotated YouTube tags, we use the Common Tag vocabulary~\cite{CommonTag:Spec}. A resource is \url{ctag:tagged} with a \url{ctag:Tag}, which consists of a textual \url{ctag:label} and a pointer to a resource that specifies what the label \url{ctag:means}. The Common Tag vocabulary is well-established and developed by both industry and academic partners.

\subsection{Entities In Video Fragments}
% DVD: the sentence 'A video fragment stretches now over a complete sentence which usually contains more than just one line in the closed captions track, which much more matches the human perception of a self-contained incident in a video' is not clear to me.
% TOM: Reformulated. Better?

As stated before the current video annotation Web service is a re-implementation of our previous client-side application SemWebVid~\cite{Steiner:SemWebVid}. Hence we already could collect some experience with modeling video data in RDF on our own, and were also inspired by the semantic video search engine yovisto\footnote{\url{http://yovisto.com/}}~\cite{Sack:Use, Sack:VideoSearch}. In our first attempt we used the Event Ontology\cite{Raimond:Event} and defined each line (which not necessarily corresponds to a complete sentence) in the closed captions track as an \url{event:Event}. In the current implementation we simplified the annotation model by removing the notion of events, and by introducing the notion of video fragments instead. We split the single lines in the complete closed captions track in complete sentences. In consequence a video fragment now stretches over a complete sentence which usually contains more than just one line in the closed captions track. This matches much more the human perception of a self-contained incident in a video. 

In order to address a video fragment, we decided to use Media Fragment URIs~\cite{W3C:MediaFrags}. Media Fragment URIs are also supported by the Ontology for Media Annotation in form of \url{ma:MediaFragment}. In particular we use the temporal dimension (e.g., \url{http://example.org/video.webm#t=10,20}), which is defined by its start and end time relative to the entire video play time. In addition to the temporal dimension, we also use the track dimension (e.g., \url{http://example.org/video.webm#track=closedcaptions}), which allows for addressing only a closed captions track (i.e., speech with time information), or even the plaintext audio transcription without time information (e.g., \texttt{\#track=audiotranscription}). The value of the parameter \texttt{track} is a free-form string, so we are flexible with regards to its usage.

In the previous SemWebVid implementation we used \url{event:factor}, \url{event:product}, and \url{event:agent} to relate events with factors (extracted non-person entities), products (the particular plaintext closed captions line), and agents (extracted persons). Now, in order to annotate entities in a temporal video fragment, we consistently use the same Common Tag vocabulary as outlined in Sect.~\ref{sec:youtube}. We thus have (in Turtle\footnote{\url{http://www.w3.org/TeamSubmission/turtle/}} syntax, left out prefixes for the sake of brefity):
\begin{lstlisting}
<http://example.org/video.webm#t=10,20> 
  a ma:MediaFragment ;
  ctag:tagged 
    [ a ctagTag ;
      ctag:label "example" ;
      ctag:means <http://example.org/example#>
    ] .
\end{lstlisting}

\section{Tracking Provenance With Multiple Data Sources}\label{sec:tracking}
As outlined before we use several data sources (Web services) in the background in order to deploy our own video annotation Web service. The simple example fact produced by our service that a \url{ma:MediaFragment} is \url{ctag:tagged} with a \url{ctagTag} with the \url{ctag:label} in plaintext form \url{example}, where what this \url{ctag:label} \url{ctag:means} is represented by an example entity with the URI \url{http://example.org/example#}, might in consequence have been the result of up to, in the concrete case, seven agreeing (or disagreeing) Web services. In order to track the contributions of the various sources, we decided to use the Provenance Vocabulary~\cite{Hartig:Provenance} by Hartig and Zhao. Even if the direct requests of our Web service were made against our wrappers (as outlined in Sect.~\ref{sec:consolidation1} and Sect.~\ref{sec:consolidation2}), we still want to credit back the results to the original calls to the third party Web services. 

We have two basic cases that affect the RDF describing the data provenance: requests per HTTP \texttt{GET} and requests per HTTP \texttt{POST}. All URI Lookup services that we use are \texttt{GET}-based, all of our NLP services are \texttt{POST}-based. In order to make statements about a bundle of triples, we group them in a named graph. We use the TriG~\cite{Bizer:TriG} syntax. The example from above then looks like this:
\begin{lstlisting}
:G = {
  <http://example.org/video.webm#t=10,20> 
    a ma:MediaFragment ;
    ctag:tagged [
      a ctagTag ;
      ctag:label "example" ;
      ctag:means <http://example.org/example#> 
    ] .  
} .
\end{lstlisting}

\subsection{The Provenance Vocabulary}\label{sec:provenance}
In the next paragraph we outline the required steps in order to make statements about the provenance of a group of triples contained in a named graph \url{:G} that were generated using several HTTP \texttt{GET} requests to third party Web services. The text below can be best understood by following the triples in Appendix~\ref{sec:appendix}.

First, we state that \url{:G} is both a \url{prv:DataItem} and obviously an \url{rdfg:Graph}. \url{:G} is \url{prv:createdBy} the process of a \url{prv:DataCreation}. This \url{prv:DataCreation} is \url{prv:performedBy} a \url{prv:NonHumanActor}, a 
\url{prvTypes:DataCreatingService} to be precise. This service is \url{prv:operatedBy} a human (\url{http://tomayac.com/thomas_steiner.rdf#me}). Time is often important for provenance, so the \url{prv:performedAt} date of the \url{prv:DataCreation} needs to be saved. During the process of the \url{prv:DataCreation} there are \url{prv:usedData}, which are \url{prv:retrievedBy} a \url{prv:DataAcess} that is \url{prv:performedAt} a certain time, and \url{prv:performedBy} a non-human actor (our Web service) that is \url{prv:operatedBy} a human (\url{http://tomayac.com/thomas_steiner.rdf#me}. For the \url{prv:DataAccess} (there is one for each third party Web service involved) we \url{prv:accessedService} from a \url{prv:DataProvidingService} of which we \url{prv:accessedResource} at a certain \url{irw:WebResource}. Therefore we \url{prvTypes:exchangedHTTPMessage} which is an \url{http:Request} using \url{http:httpVersion} ``1.1'' and the \url{http:methodName} ``GET''.

\subsection{Tracking Provenance With Human Interaction}\label{sec:human}
Oftentimes completely automatically generated RDF video annotation files will need a bit of manual fine-tuning. In our RESTful Web service we have thus envisiond that not only a big archive of automatically generated video annotations gets built, but that also people can correct errors (via HTTP \texttt{PUT} or later \texttt{PATCH}) in the RDF interactively (or remove completely wrong video annotations via HTTP \texttt{DELETE}). For the correction case this can be tracked using the Provenance Vocabulary as follows (let us assume we wanted to replace an unfriendly Freebase \url{ctag:means} resource of \url{http://rdf.freebase.com/ns/guid.9202a8c04000641f800000000042acea} with the friendlier variant \url{http://rdf.freebase.com/rdf/en.google}):
\begin{lstlisting}
:G_corrected = {
  <http://gdata.youtube.com/feeds/api/videos/3PuHGKnboNY> 
  ctag:tagged [
    a ctag:Tag ;
    ctag:label "google" ;
    ctag:means <http://rdf.freebase.com/rdf/en.google> 
  ]  
} .
:G_corrected
  a prv:DataItem ;
  a rdfg:Graph ;
  prv:createdBy [
    a prv:DataCreation ;
    prv:performedAt "2011-02-07T12:42:30Z"^^xsd:dateTime ;
    prv:performedBy [
      a prv:HumanActor ;
      <http://tomayac.com/thomas_steiner.rdf#me> 
    ] 
  ] .
\end{lstlisting}

Note how the \url{prv:DataCreation} no longer contains references to \url{prv:usedData}. Obviously the shown approach to identify a \url{prv:HumanActor} with her FOAF profile requires an authentication step, which might not always be wanted. One could think of a ``John Doe''\footnote{\url{http://en.wikipedia.org/wiki/John_Doe}}-like anonymous pseudo-\url{prv:HumanActor}, or in the case of an intendedly non-anonymous \url{prv:HumanActor}, authentication methods like WebID\footnote{\url{http://www.w3.org/2005/Incubator/webid/charter}} could be used.

\subsection{The Need For Providing Provenance Metadata}
Hartig et al. mention in~\cite{ipaw10:olaf} some reasons that justify the need for provenance metadata, among those linked dataset replication and distribution on the Web with not necessarily always the same namespaces: based on the same source data, different copies of a linked dataset can be created with different degrees of interconnectedness by different publishers.

We add to this list the automatic conversion of legacy unstructured data to Linked Data with heuristics, where, as in our case, extracted entities while being consolidated and backed up by different data sources, might still be wrong. Especially with our ``mash-up''-like approach it is very desirable to be able to track back to the concrete source where a certain piece of information might have come from. This a) in order to correct the error at the root of our Web service (fighting the cause), b) in order to correct the concrete error in an RDF annotation (fighting the symptom), or c) probably the most important reason, to judge the trustworthiness and quality of a dataset.

% DVD: what if DBPedia or other data sets change over time? I.e., the whole discussion regarding dataset dynamics in the context of provenance comes in the picture here. Therefore, the data sources themselves should also provide provenance information in order to enable a real working quality checker.
% TOM: Added a subsection to address this remark.

\subsection{Change Of Referenced Datasets Over Time}\label{sec:change}
It is to be noted that a statement like the triple below refers to the triple object as an identificator for a Web resource (where the Web resource is a representation of the result of the API call at the time where it was \url{prv:performedAt}).
\begin{lstlisting}
_:x prv:accessedResource <http://api.freebase.com/api/service/search?format=json&query=obama> ;
\end{lstlisting}
As provenance metadata always refer to the time context in which a certain statement was made, it is essentially unimportant what representation the resource returns at a later time.

\section{Related And Future Work}\label{sec:related}
Related work includes Popcorn.js from the Mozilla Drumbeat project~\cite{Drumbeat:Popcorn} that with its interactive Butter editor\footnote{\url{http://popcornjs.org/butter/}} allows for a video to be semantically annotated (a completely manual process). Based on a given video annotation the Popcorn.js script then pulls in multiple data feeds from the APIs of Google News, Wikipedia, Twitter, and Flickr in order to semantically enrich the video viewing experience. It also provides automatic machine translation from Google Translate, and attribution data from Creative Commons. The focus, however, is on the final visual video ``mash-up'', not on the actual annotation. Future work could be to offer an RDF-to-Popcorn.js wrapper service that would allow us to profit from the project's HTML5 video framework.

In~\cite{Sack:VideoSearch}, Waitelonis et al. address the problem of how to deploy exploratory search for video data by using semantic search technology for the yovisto video search engine. They show how exploratory search can be enriched by information from the LOD cloud in order to facilitate navigation in big video archives. Yovisto supports several ways to annotate a video with metadata: video-related, and video-time-related tags. Video-related tags are applied to the entire video and are entered by the initial video uploader, whereas video-time-related tags only apply to a certain point in the video. They can either be automatically extracted from the video on a certain timestamp (e.g., by analyzing the video images with OCR methods), or can be user-generated tags also on a certain timestamp. In a different paper~\cite{Sack:Use} Waitelonis et al. show how using permutations of a term and this term's surrounding context and by detecting paths between entities, a legacy keyword-based video search engine can be converted into a semantic video search engine. The approach uses a keyword-to-DBpedia-URI mapping heuristic, however, as far as we can tell, provenance metadata is not maintained. Future work will compare the results of the yovisto heuristic with ours using agreed-on benchmarks.

In~\cite{Choudhury:YouTube} Choudhury et al. describe a framework for semantic enrichment, ranking, and integration of Web video tags using Semantic Web technologies. In order to enrich the oftentimes sparse user-generated tag space, meta data like the recording time and location, or the video title and video description are used, but also social features such as playlists that a video appears in and related videos. Next, the tags are ranked by their co-occurrence and in a final step interlinked to DBpedia concepts for greater integration with other datasets. Choudhury et al. disambiguate the tags based on WordNet\footnote{\url{http://wordnet.princeton.edu/}} synsets if possible. That means if there is only one matching synset in WordNet, the corresponding WordNet URI in DBpedia is selected. If there are more than one matching synsets, the tags and their context tags similarity is computed and thereby tried to decide on an already existing tag URI. For words that are not contained in WordNet, Sindice is used to find the most probable concept. To the best of our knowledge provenance metadata is not maintained.

\section{Conclusion}\label{sec:conclusion}
We have introduced a Web service for semantic text-based video annotation for YouTube videos with closed captions. Therefore we presented several URI Lookup and NLP Web services and showed our approach for both classes of Web services to consolidate entities. We then focused on the necessary RDF vocabularies and Media Fragment URIs to annotate video-related and video-time-related entities. Due to their different ``mash-up''-like history of origins, we need to track provenance metadata in order to assure the trustworthiness of the generated data. We showed how the Provenance Vocabulary can be used to keep track of even the original third party Web service calls that led to consolidated results. It is to be noted that these references to the original calls are to be understood as the identificator of Web resources (i.e., the results of a request). Finally we positioned our work to related work, and presented directions for future work.

In this paper we have shown how a concrete multi-source RESTful Web service can automatically maintain provenance metadata, both for entirely machine-generated content, but also for partly (or completely) human-generated content. We believe that being able to track back the origin of a triple is of immense importance, especially given the network effect which is one of the Linked Data benefits.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}\label{sec:acknowledgments}
We would like to thank Olaf Hartig from the Humboldt-Universit\"{a}t zu Berlin for his kind support with the correct use of the Provenance Vocabulary. This work is partly funded by the EU FP7 I-SEARCH project (project reference 248296).

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc-sp}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\pagebreak
\appendix
%Appendix A
\section{Provenance RDF Overview}\label{sec:appendix}
Shortened overview of the provenance RDF in Turtle syntax for a YouTube tag with the label ``obama'' and the assigned meaning \url{http://dbpedia.org/resource/Barack_Obama} (for the sake of brefity only two of the \url{prv:usedData} sources are mentioned):
\begin{lstlisting}
:G = {
  <http://gdata.youtube.com/feeds/api/videos/3PuHGKnboNY> ctag:tagged :tag .
  :tag
    a ctag:Tag ;
    ctag:label "obama" ;
    ctag:means <http://dbpedia.org/resource/Barack_Obama> ;
} .
:G
  a prv:DataItem ;
  a rdfg:Graph ;
  prv:createdBy [
    a prv:DataCreation ;
    prv:performedAt "2011-02-07T12:42:30Z"^^xsd:dateTime ;
    prv:performedBy [
      a prv:NonHumanActor ;
      a prvTypes:DataCreatingService ;
      prv:operatedBy <http://tomayac.com/thomas_steiner.rdf#me> .
    ] ;
    prv:usedData [
      prv:retrievedBy [
        a prv:DataAcess ;
        prv:performedAt "2011-02-07T12:42:30Z"^^xsd:dateTime ;
        prv:performedBy [
          prv:operatedBy <http://tomayac.com/thomas_steiner.rdf#me> .
        ] ;
        prv:accessedService <http://api.freebase.com/api/service/search> ;
        prv:accessedResource <http://api.freebase.com/api/service/search?format=json&query=obama> ;
        prvTypes:exchangedHTTPMessage [
          a http:Request ;
          http:httpVersion "1.1" ;
          http:methodName "GET" ;
          http:mthd <http://www.w3.org/2008/http-methods#GET> ;
          http:headers (
            [
              http:fieldName "Host" ;
              http:fieldValue "api.freebase.com" ;
              http:hdrName <http://www.w3.org/2008/http-header#host> ;
            ]
          )
        ] ;
      ] ;
    ] ;
    prv:usedData [
      prv:retrievedBy [
        a prv:DataAcess ;
        prv:performedAt "2011-02-07T12:42:30Z"^^xsd:dateTime ;
        prv:performedBy [
          prv:operatedBy <http://tomayac.com/thomas_steiner.rdf#me> .
        ] ;
        prv:accessedService <http://lookup.dbpedia.org/> ;
        prv:accessedResource <http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?QueryString=obama> ;
        prvTypes:exchangedHTTPMessage [
          a http:Request ;
          http:httpVersion "1.1" ;
          http:methodName "GET" ;
          http:mthd <http://www.w3.org/2008/http-methods#GET> ;
          http:headers (
            [
              http:fieldName "Host" ;
              http:fieldValue "lookup.dbpedia.org" ;
              http:hdrName <http://www.w3.org/2008/http-header#host> ;
            ]
          )
        ] ;
      ] ;
    ] ;
  ] .
} .
\end{lstlisting}

\balancecolumns
\end{document}
